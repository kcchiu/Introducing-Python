{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "    TEMPLATE FOR MACHINE LEARNING HOMEWORK\n",
    "    AUTHOR Eric Eaton, Vishnu Purushothaman Sreenivasan\n",
    "'''\n",
    "\n",
    "import numpy as np\n",
    "from sklearn import tree\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "class BoostedDT:\n",
    "\n",
    "    def __init__(self, numBoostingIters=100, maxTreeDepth=3):\n",
    "        '''\n",
    "        Constructor\n",
    "        '''\n",
    "        #TODO\n",
    "        self.numBoostingIters = numBoostingIters\n",
    "        self.maxTreeDepth = maxTreeDepth\n",
    "        self.w = 0\n",
    "        self.Pred = 0\n",
    "        self.Beta = 0\n",
    "        self.model = np.empty((numBoostingIters),dtype=object)\n",
    "        self.K = 0    \n",
    "\n",
    "    def fit(self, X, y):\n",
    "        '''\n",
    "        Trains the model\n",
    "        Arguments:\n",
    "            X is a n-by-d numpy array\n",
    "            y is an n-dimensional numpy array\n",
    "        '''\n",
    "        #TODO\n",
    "        self.K = np.unique(y).shape[0] #3, # of calsses\n",
    "        numInst, numFeat = X.shape  \n",
    "        self.w = np.ones(numInst) / numInst\n",
    "        epsilon = 0\n",
    "        pred = np.zeros([self.numBoostingIters,numInst])\n",
    "        beta = np.zeros([self.numBoostingIters])\n",
    "        \n",
    "        for i in range(self.numBoostingIters):\n",
    "            treeBoost = tree.DecisionTreeClassifier(criterion='entropy',\n",
    "                                               max_depth=self.maxTreeDepth)\n",
    "            treeBoost.fit(X,y,sample_weight=self.w)\n",
    "            pred[i,:] = treeBoost.predict(X)\n",
    "            wError = np.array([self.w[j] if pred[i,j] != y[j] else 0 for j in np.arange(numInst)]) # numInst x numFeat # the entries are the corresponding weight of misclassified instances, else 0.\n",
    "            epsilon = wError.sum()\n",
    "            beta[i] = 0.5 * (np.log((1-epsilon)/epsilon) + np.log(self.K-1))\n",
    "#            self.w = np.array([self.w[j]*np.exp(-beta[i]) if pred[i,j] == y[j] else self.w[j]*np.exp(beta[i]) for j in np.arange(numInst)]) # numInst x numFeat # if predicted correctly, weight = weight x exp(-beta), else  weight = weight x exp(beta).\n",
    "            self.w = np.array([self.w[j] if pred[i,j] == y[j] else self.w[j]*np.exp(beta[i]) for j in np.arange(numInst)]) # numInst x numFeat # if predicted correctly, weight = weight x 1, else  weight = weight x exp(beta).\n",
    "            self.w = self.w / self.w.sum() # normalize the weights to sum to 1\n",
    "#            print beta[i], accuracy_score(pred[i,:],y)\n",
    "            self.model[i] = treeBoost\n",
    "        self.Pred = pred\n",
    "        self.Beta = beta\n",
    "\n",
    "    def predict(self, X):\n",
    "        '''\n",
    "        Used the model to predict values for each instance in X\n",
    "        Arguments:\n",
    "            X is a n-by-d numpy array\n",
    "        Returns:\n",
    "            an n-dimensional numpy array of the predictions\n",
    "        '''\n",
    "        #TODO\n",
    "        rtn = 3*np.ones([X.shape[0]])\n",
    "        arg = np.zeros([self.K])\n",
    "        \n",
    "        for j in range(X.shape[0]):\n",
    "            for guess in range(self.K):\n",
    "                arg[guess] = np.array([self.Beta[i] if self.model[i].predict(X[j,:].reshape(1,-1)) == guess else 0 for i in range(self.numBoostingIters)]).sum()\n",
    "            rtn[j] = np.argmax(arg) # return the prdicted class (0, 1 or 2) which has the max arg value\n",
    "#            print rtn[j]\n",
    "        return rtn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================\n",
      "Test the boostedDT against the standard decision tree\n",
      "======================================================\n",
      "\n",
      "Author: Eric Eaton, 2014\n",
      "\n",
      "\n",
      "numBoostingIters: 100, maxTreeDepth: 1\n",
      "CV accuracy: 0.464, +-0.006\n",
      "%s\n",
      "numBoostingIters: 100, maxTreeDepth: 2\n",
      "CV accuracy: 0.619, +-0.156\n",
      "%s\n",
      "numBoostingIters: 100, maxTreeDepth: 3\n",
      "CV accuracy: 0.692, +-0.164\n",
      "%s\n",
      "numBoostingIters: 100, maxTreeDepth: 4\n",
      "CV accuracy: 0.732, +-0.158\n",
      "%s\n",
      "numBoostingIters: 100, maxTreeDepth: 5\n",
      "CV accuracy: 0.758, +-0.151\n",
      "%s\n",
      "numBoostingIters: 500, maxTreeDepth: 1\n",
      "CV accuracy: 0.735, +-0.147\n",
      "%s\n",
      "numBoostingIters: 500, maxTreeDepth: 2\n",
      "CV accuracy: 0.746, +-0.139\n",
      "%s\n",
      "numBoostingIters: 500, maxTreeDepth: 3\n",
      "CV accuracy: 0.759, +-0.134\n",
      "%s\n",
      "numBoostingIters: 500, maxTreeDepth: 4\n",
      "CV accuracy: 0.771, +-0.131\n",
      "%s\n",
      "numBoostingIters: 500, maxTreeDepth: 5\n",
      "CV accuracy: 0.780, +-0.128\n",
      "%s\n",
      "numBoostingIters: 1000, maxTreeDepth: 1\n",
      "CV accuracy: 0.766, +-0.130\n",
      "%s\n",
      "numBoostingIters: 1000, maxTreeDepth: 2\n",
      "CV accuracy: 0.770, +-0.125\n",
      "%s\n",
      "numBoostingIters: 1000, maxTreeDepth: 3\n",
      "CV accuracy: 0.776, +-0.122\n",
      "%s\n",
      "numBoostingIters: 1000, maxTreeDepth: 4\n",
      "CV accuracy: 0.782, +-0.120\n",
      "%s\n",
      "numBoostingIters: 1000, maxTreeDepth: 5\n",
      "CV accuracy: 0.788, +-0.118\n",
      "%s\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "======================================================\n",
    "Test the boostedDT against the standard decision tree\n",
    "======================================================\n",
    "\n",
    "Author: Eric Eaton, 2014\n",
    "\n",
    "\"\"\"\n",
    "print(__doc__)\n",
    "\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from numpy import loadtxt\n",
    "\n",
    "\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# load the data set\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "#===============================================\n",
    "filename = 'data/challengeTrainLabeled.dat'\n",
    "data = loadtxt(filename, delimiter=',')\n",
    "X = data[:,0:10]\n",
    "y = np.array([data[:, -1]]).T\n",
    "#===============================================\n",
    "#\n",
    "#n,d = X.shape\n",
    "#nTrain = int(0.5*n)  #training on 50% of the data\n",
    "#\n",
    "## shuffle the data\n",
    "#idx = np.arange(n)\n",
    "#np.random.seed(13)\n",
    "#np.random.shuffle(idx)\n",
    "#X = X[idx]\n",
    "#y = y[idx]\n",
    "#\n",
    "## split the data\n",
    "#Xtrain = X[:nTrain,:]\n",
    "#ytrain = y[:nTrain]\n",
    "#Xtest = X[nTrain:,:]\n",
    "#ytest = y[nTrain:]\n",
    "#\n",
    "## train the decision tree\n",
    "#modelDT = DecisionTreeClassifier()\n",
    "#modelDT.fit(Xtrain,ytrain)\n",
    "#\n",
    "### train the boosted DT\n",
    "##modelBoostedDT = BoostedDT(numBoostingIters=1000, maxTreeDepth=4)\n",
    "##modelBoostedDT.fit(Xtrain,ytrain)\n",
    "#\n",
    "## output predictions on the remaining data\n",
    "#ypred_DT = modelDT.predict(Xtest)\n",
    "##ypred_BoostedDT = modelBoostedDT.predict(Xtest)\n",
    "#\n",
    "## compute the training accuracy of the model\n",
    "#accuracyDT = accuracy_score(ytest, ypred_DT)\n",
    "##accuracyBoostedDT = accuracy_score(ytest, ypred_BoostedDT)\n",
    "#\n",
    "#print \"Decision Tree Accuracy = \"+str(accuracyDT)\n",
    "##print \"Boosted Decision Tree Accuracy = \"+str(accuracyBoostedDT)\n",
    "#\n",
    "#numBoostingIters_range = [100]\n",
    "#maxTreeDepth_range = [2,3,4]\n",
    "#\n",
    "##pipe = Pipeline([('bt', BoostedDT)])\n",
    "##param_grid = [{'bt__numBoostingIters': numBoostingIters_range,\n",
    "##               'bt__maxTreeDepth':maxTreeDepth_range}]\n",
    "##gs = GridSearchCV(estimator=pipe,\n",
    "##                  param_grid=param_grid,\n",
    "##                  scoring='accuracy',\n",
    "##                  cv=2,\n",
    "##                  n_jobs=1,\n",
    "##                  verbose=10)\n",
    "##gs = gs.fit(Xtrain,ytrain)\n",
    "##print(gs.best_score_)\n",
    "##print(gs.best_params_)\n",
    "\n",
    "numBoostingIters_range = [100, 500, 1000]\n",
    "maxTreeDepth_range = [1,2,3,4,5]\n",
    "\n",
    "y = y.reshape([y.shape[0],])\n",
    "\n",
    "from sklearn.cross_validation import StratifiedKFold\n",
    "kfold = StratifiedKFold(y=y,\n",
    "                        n_folds=5)\n",
    "scores = []\n",
    "\n",
    "for numBoostingIters in numBoostingIters_range:\n",
    "    for maxTreeDepth in maxTreeDepth_range:\n",
    "        print (\"numBoostingIters: %d, maxTreeDepth: %d\" % (numBoostingIters, maxTreeDepth))\n",
    "        for k, (train, test) in enumerate(kfold):    \n",
    "            modelBoostedDT = BoostedDT(numBoostingIters=numBoostingIters, maxTreeDepth=maxTreeDepth)\n",
    "            modelBoostedDT.fit(X[train],y[train])\n",
    "            ypred_BoostedDT = modelBoostedDT.predict(X[test])\n",
    "            accuracyBoostedDT = accuracy_score(y[test], ypred_BoostedDT)\n",
    "            scores.append(accuracyBoostedDT)\n",
    "#            print \"Fold:\", k+1, \"Class dist:\", np.bincount(y[train]), \"Accuracy:\", accuracyBoostedDT\n",
    "#            print \"Fold:\", k+1, \"Accuracy:\", accuracyBoostedDT\n",
    "#            print \"Boosted Decision Tree Accuracy = \"+str(accuracyBoostedDT)\n",
    "        print (\"CV accuracy: %.3f, +-%.3f\" % (np.mean(scores), np.std(scores)))\n",
    "        print (\"========================\")\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================\n",
      "Test the boostedDT against the standard decision tree\n",
      "======================================================\n",
      "\n",
      "Author: Eric Eaton, 2014\n",
      "\n",
      "\n",
      "numBoostingIters: 1000, maxTreeDepth: 3\n",
      "CV accuracy: 0.850, +-0.010\n",
      "========================\n",
      "numBoostingIters: 1000, maxTreeDepth: 4\n",
      "CV accuracy: 0.862, +-0.016\n",
      "========================\n",
      "numBoostingIters: 1000, maxTreeDepth: 5\n",
      "CV accuracy: 0.864, +-0.014\n",
      "========================\n",
      "numBoostingIters: 1000, maxTreeDepth: 6\n",
      "CV accuracy: 0.869, +-0.010\n",
      "========================\n",
      "numBoostingIters: 1000, maxTreeDepth: 7\n",
      "CV accuracy: 0.871, +-0.008\n",
      "========================\n",
      "numBoostingIters: 1500, maxTreeDepth: 3\n",
      "CV accuracy: 0.849, +-0.009\n",
      "========================\n",
      "numBoostingIters: 1500, maxTreeDepth: 4\n",
      "CV accuracy: 0.860, +-0.017\n",
      "========================\n",
      "numBoostingIters: 1500, maxTreeDepth: 5\n",
      "CV accuracy: 0.866, +-0.013\n",
      "========================\n",
      "numBoostingIters: 1500, maxTreeDepth: 6\n",
      "CV accuracy: 0.869, +-0.010\n",
      "========================\n",
      "numBoostingIters: 1500, maxTreeDepth: 7\n",
      "CV accuracy: 0.871, +-0.009\n",
      "========================\n",
      "numBoostingIters: 2000, maxTreeDepth: 3\n",
      "CV accuracy: 0.850, +-0.007\n",
      "========================\n",
      "numBoostingIters: 2000, maxTreeDepth: 4\n",
      "CV accuracy: 0.860, +-0.015\n",
      "========================\n",
      "numBoostingIters: 2000, maxTreeDepth: 5\n",
      "CV accuracy: 0.865, +-0.014\n",
      "========================\n",
      "numBoostingIters: 2000, maxTreeDepth: 6\n",
      "CV accuracy: 0.870, +-0.009\n",
      "========================\n",
      "numBoostingIters: 2000, maxTreeDepth: 7\n",
      "CV accuracy: 0.869, +-0.009\n",
      "========================\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "======================================================\n",
    "Test the boostedDT against the standard decision tree\n",
    "======================================================\n",
    "\n",
    "Author: Eric Eaton, 2014\n",
    "\n",
    "\"\"\"\n",
    "print(__doc__)\n",
    "\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from numpy import loadtxt\n",
    "\n",
    "\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# load the data set\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "#===============================================\n",
    "filename = 'data/challengeTrainLabeled.dat'\n",
    "data = loadtxt(filename, delimiter=',')\n",
    "X = data[:,0:10]\n",
    "y = np.array([data[:, -1]]).T\n",
    "#===============================================\n",
    "\n",
    "numBoostingIters_range = [1000, 1500, 2000]\n",
    "maxTreeDepth_range = [3,4,5,6,7]\n",
    "\n",
    "y = y.reshape([y.shape[0],])\n",
    "\n",
    "from sklearn.cross_validation import StratifiedKFold\n",
    "kfold = StratifiedKFold(y=y,\n",
    "                        n_folds=5)\n",
    "scores = []\n",
    "\n",
    "for numBoostingIters in numBoostingIters_range:\n",
    "    scores = []\n",
    "    for maxTreeDepth in maxTreeDepth_range:\n",
    "        print (\"numBoostingIters: %d, maxTreeDepth: %d\" % (numBoostingIters, maxTreeDepth))\n",
    "        scores = []\n",
    "        for k, (train, test) in enumerate(kfold):    \n",
    "            modelBoostedDT = BoostedDT(numBoostingIters=numBoostingIters, maxTreeDepth=maxTreeDepth)\n",
    "            modelBoostedDT.fit(X[train],y[train])\n",
    "            ypred_BoostedDT = modelBoostedDT.predict(X[test])\n",
    "            accuracyBoostedDT = accuracy_score(y[test], ypred_BoostedDT)\n",
    "            scores.append(accuracyBoostedDT)\n",
    "#            print \"Fold:\", k+1, \"Class dist:\", np.bincount(y[train]), \"Accuracy:\", accuracyBoostedDT\n",
    "#            print \"Fold:\", k+1, \"Accuracy:\", accuracyBoostedDT\n",
    "#            print \"Boosted Decision Tree Accuracy = \"+str(accuracyBoostedDT)\n",
    "        print (\"CV accuracy: %.3f, +-%.3f\" % (np.mean(scores), np.std(scores)))\n",
    "        print (\"========================\")\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
